%!TEX root = Main.tex
\section*{Probabilistic Generative Models}
This part of the article, describes the Probabilistic Generative models (PGM).
The assumed probabilistic model for each class is a multivariate normal distribution. 
\begin{equation}
p(\mathbf{x}|C_k)=
\mathcal{N}(\mathbf{x};\mathbf{\mu}_k, \; \Sigma_k) 
\label{eq:gauss_dist} 
\end{equation}

\subsection*{Training}
In the training process the mean vector and covariance matrix of each class is estimated.

\begin{equation}
\bm{\mu}_k= \dfrac{1}{N} \sum_{j=1}^{N} \mathbf{x}_{kj} \\
\end{equation}

\begin{equation}
\bm{\Sigma}_k =
\dfrac{1}{N} 
\sum_{j=1}^{N} 
	(\mathbf{x}_{kj}-\bm{\mu}_k) 
	\cdot 
	(\mathbf{x}_{kj}-\bm{\mu}_k)
\end{equation}
The mean and covariance from the training is used to calculate the probability density for each class.
\begin{equation}
p(\mathbf{x}|C_k)=  
\dfrac{1}{(2\pi)^{D/2}} \dfrac{1}{\left|\mathbf{\Sigma} \right|^{1/2}} 
e^{
	-\dfrac{1}{2} 
	(\mathbf{x}-\bm{\mu}_k)^T 
	\bm{\Sigma}^{-1}
	(\mathbf{x}-\bm{\mu}_k) 
}
\end{equation}

The class prior is selected as an equal likelihood for each class. The probability of a given feature vector $ \mathbf{x} $ being part of a class $ C_k $ is given by.
\begin{equation}
p(C_k |\mathbf{x}) =
\dfrac{p(\mathbf{x}|C_k) p(C_k)}
{\sum_j p(\mathbf{x}|C_j) p(C_j)}
\end{equation}

\subsection*{Result}
The result of the probabilistic generative classifier is a total accuracy of 62.7, 58.1 and 57.2 \% respectively for one, two and ten digits. 