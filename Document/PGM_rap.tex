\section*{Probabilistic Generative Models}
This part of the article, describes the Probabilistic Generative models (PGM).
The assumed probabilistic model for each class is a multivariate normal distribution. 
\begin{equation}
p(\mathbf{x}|C_k)=
\mathcal{N}(\mathbf{x};\mathbf{\mu}_k, \; \Sigma_k) 
\label{eq:gauss_dist} 
\end{equation}

\paragraph*{Training}
In the training process the mean vector and covariance matrix of each class is estimated.
\begin{eqnarray}
\bm{\mu}_i= \dfrac{1}{N} \sum_{j=1}^{N} \mathbf{x}_{kj} \\
\bm{\Sigma}_k =\dfrac{1}{N} \sum_{j=1}^{N} (\mathbf{x}_{kj}-\bm{\mu}_k) \cdot (\mathbf{x}_{kj}-\bm{\mu}_k)
\end{eqnarray}
The mean and covariance from the training is used to calculate the probability density for each class.
\begin{equation}
p(\mathbf{x}|C_k)=  
\dfrac{1}{(2\pi)^{D/2}} \dfrac{1}{\left|\mathbf{\Sigma} \right|^{1/2}} 
\mathtt{exp} \left\lbrace -\dfrac{1}{2} (\mathbf{x}-\mathbf{\mu}_k)^T \mathbf{\Sigma}^{-1} (\mathbf{x}-\mathbf{\mu}_k) \right\rbrace
\end{equation}

The class prior is selected as an equal likelihood for each class. The probability of a given feature vector $ \mathbf{x} $ being part of a class $ C_k $ is given by.
\begin{equation}
p(C_k |\mathbf{x}) =
\dfrac{p(\mathbf{x}|C_k) p(C_k)}
{\sum_j p(\mathbf{x}|C_j) p(C_j)}
\end{equation}

\paragraph*{Result}
The result of the probabilistic generative classifier is a total accuracy of 62.7, 58.1 and 57.2 \% respectively for one, two and ten digits. 