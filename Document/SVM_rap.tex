%!TEX root = Main.tex
\section*{Support Vector Machines}
In this section the soft margin support vector machine (SVM) is described. 
The SVM is a binary classifier, which is a problem with multi class. 
The solution for multiclass classification is using a one-vs.-one setup.
The decision function for a standard SVM for a test point $ x_{new} $ is given by
\begin{equation}
t_{new} = \mathtt{sign}(\mathbf{w}^T \mathbf{x}_{new} +b)
\label{eq:SVM_lin}
\end{equation}

\subsection*{Training}
The parameter vector $ \mathbf{w} $ is found by maximizing the margin or minimizing the length of the parameter vector, because of the inverse relationship $ \gamma = \frac{1}{\|\mathbf{w}\|} $.
In the training process of a soft margin SVM, the following expression is minimized. 
\begin{align}
\mathbf{w}^* = 
\mathtt{argmin}_\mathbf{w} \frac{1}{2} \mathbf{w}^T \mathbf{w}+C \sum_{n=1}^{N} \xi_n,\\ \mathtt{w.r.t.} \qquad t_n(\mathbf{w}^T \mathbf{x}_n + b) \geq 1-\xi_n
\end{align} 
The influence of each training point in the decision boundary is proportional to $ \alpha_n $.
After the training process, the classification can be done by using the following expression, where $ \mathbf{x}_n $  are the support-vectors, and $ \alpha_n $ is their weights.
\begin{equation}
t_{new} = 
\mathtt{sign}\left( \sum_{n=1}^{N} \alpha_n t_n k(\mathbf{x}_n,\mathbf{x}_{new}) +b  \right)
\end{equation}
with $ k = \mathtt{exp}(-\gamma \|\mathbf{x}^T_n - \mathbf{x}_{new} \|^2 ) $, which is the Gaussian kernel.

\subsection*{Results}
The result was obtained by using a Gaussian kernel, and the classification accuracy was found to be 73.7 \% for the one digit dataset after training on the training set and afterwards classifying the test set.