\chapter{Linear classification}

\section{Theory}

In linear classification the decision surfaces are linear functions of the input vector $\mathbf{b}$. 
The decision surfaces are defined by D-1 dimensional hyperplane in the D dimensional input space.
the target of the classification is labelled in the target variable $\mathbf{t}$, using the target values to represent class labels. 
In the case of two-class problems, a single target variable can be represented by $t\in \lbrace 0,1\rbrace$ where $t = 1$ represents class $C_1$ and $t = 0$ represents class $C_2$.
If the is more than 2 classes $(K>2)$, then $\mathbf{t}$ is a vector of length K.
For Class $C_j$ the element $t_j$ takes value 1 and all other elements $t_k$ of $\mathbf{t}$ are zero.
In this project there are 3 different speakers, $K = 3$ then the target vector for class 3 be $\mathbf{t} = (0, 0, 1)^T$.
The value of $t_k$ can be interpreted as the probability of the given class being class $C_k$.
To assign each vector $\mathbf{x}$ with a specific class, a number of different approaches can used to classify.
One way is using a discriminant function, with a linear predictor $y(\mathbf{x},\mathbf{w})$ given by the parameter $\mathbf{w}$ and the input vector set $\mathbf{x}=(x_1,...,x_D)^T$, which is linear. 
The linear discriminant function in its simplest form

\begin{equation}
y(\mathbf{x}) = \mathbf{w}^T \mathbf{x}+w_0
\label{eq:lineDis}
\end{equation}

Where $w_0$ is the bias and $\mathbf{w}$ is the weight vector.
The decision boundary corresponds to $y(\mathbf{x})=constant$, and hence $\mathbf{w}^T \mathbf{x}+w_0 = constant$ therefore the decision boundary is a linear function.






\section{Method}
The process to apply the linear classifier to the dataset, is described below. 
The result of the linear classifier is evaluated by applying a confusion matrix and calculating the accuracy. 
The linear classifier is applied to the training dataset, containing feature vectors $(\mathbf{x}_n)$ and the target vectors $(\mathbf{t}_n)$.
The vectors are on the form:
\begin{equation}
\mathbf{\tilde{X}}=\left[ \begin{array}{c}\mathbf{x}_1^T 1\\
\mathbf{x}_2^T 1\\
...\\ 
\mathbf{x}_n^T 1 \end{array} \right],
\;
\mathbf{T}=\left[ \begin{array}{c}
\mathbf{t}_1^T\\ 
\mathbf{t}_2^T\\ 
...\\
\mathbf{t}_n^T
\end{array} \right]
\label{eq:linearVectors}  
\end{equation} 

To determine the weight matrix $\tilde{\mathbf{W}}$:
\begin{equation}
\tilde{\mathbf{W}} = \tilde{\mathbf{X}}^\dagger \mathbf{T} \approx  (\tilde{\mathbf{X}}^T \tilde{\mathbf{X}}+\mathbf{I})^{-1} \tilde{\mathbf{X}}^T\mathbf{T}
\label{eq:weightVector}  
\end{equation}

The are a problem with the entity $\tilde{\mathbf{X}}^T \tilde{\mathbf{X}}$ having small eigenvalues, the solution is to add the identity matrix in the calculations of the pseudo inverse matrix $\tilde{\mathbf{X}}^\dagger$.
This is done as a regularization, which can be seen as a penalty for large values in $\tilde{\mathbf{W}}$.\\

When $\tilde{\mathbf{W}}$ is calculated from the training set, it is tested on the test dataset containing feature vectors and their corresponding targets. 
The result of this equation
\begin{equation}
\mathbf{y}(\mathbf{x}) = \tilde{\mathbf{W}}^{T} \tilde{x},
\;
\tilde{x} = \left[\begin{array}{c}
\mathbf{x}\quad 1
\end{array} \right] 
\label{eq:Yclassifier}
\end{equation}

shows the most likely class for each of the given feature vectors. 
The resulting vector is placed in a vector $\mathbf{t}_{est}$, which contains the classification and ranges over the three classes $\left\lbrace 1,2,3\right\rbrace $. 

To evaluate the linear classifier, the confusion matrix is an ideal way of doing so.
The confusion matrix can show the classifications sensitivity, precision and accuracy for each class. 
The terms used to describe this is: false positive (FP), true positive (TP), false negative (FN) and true negative (TN).
The true terms are classes that are correctly classified and false terms are incorrectly classified.
The sensitivity is the probability of classifying the inputs as class $X$ for a input that are class $X$.
\begin{equation}
\mathtt{sensitivity}(X) = \dfrac{TP_X}{TP_X+\Sigma FN_X}
\label{eq:sensitivity}
\end{equation}
The precision is the probability for the estimated of class $X$ is correct.
\begin{equation}
\mathtt{precision}(X) = \dfrac{TP_X}{TP_X+\Sigma FP_X}
\label{eq:precision}
\end{equation}
The accuracy is the probability that the classification of any given class is correct, where N = total number of tests.
\begin{equation}
\mathtt{accuray}(X) = \dfrac{\Sigma TP_X}{\mathtt{N}}
\label{eq:accuracy}
\end{equation}



\section{Results}
\subsection{Single digit:}

\begin{figure}[H]
\centering
\includegraphics{Linear_1digit_8cent_3speak}
\caption{Results of using linear classifiers and single digit spoken}
\label{fig:Lin_fig_1}
\end{figure}

\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 2041.0 & 557.0 & 699.0 & 61.9 \\                   
\hline                                                              
Estimate Mose & 562.0 & 1816.0 & 820.0 & 56.8 \\                    
\hline                                                              
Estimate Simon & 851.0 & 1081.0 & 1935.0 & 50.0 \\                  
\hline                                                              
Sensitivity [\%] & 59.1 & 52.6 & 56.0 & 55.9 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 1 digits}                               
\label{table:Lin_conf_1}                                            
\end{table}    
                   

\subsection{2 digits:}

\begin{figure}[H]
\centering
\includegraphics{Linear_2digit_8cent_3speak}
\caption{Results of using linear classifiers and two digits spoken}
\label{fig:Lin_fig_2}
\end{figure}

\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 3169.0 & 830.0 & 1535.0 & 57.3 \\                  
\hline                                                              
Estimate Mose & 2163.0 & 3896.0 & 1818.0 & 49.5 \\                  
\hline                                                              
Estimate Simon & 1578.0 & 2184.0 & 3557.0 & 48.6 \\                 
\hline                                                              
Sensitivity [\%] & 45.9 & 56.4 & 51.5 & 51.2 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 2 digits}                               
\label{table:Lin_conf_2}                                            
\end{table}                                


\subsection{10 digits:}

\begin{figure}[H]
\centering
\includegraphics{Linear_10digit_8cent_3speak}
\caption{Results of using linear classifiers and ten digits spoken}
\label{fig:Lin_fig_10}
\end{figure}

\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 16346.0 & 7356.0 & 6220.0 & 54.6 \\                
\hline                                                              
Estimate Mose & 8902.0 & 15435.0 & 7615.0 & 48.3 \\                 
\hline                                                              
Estimate Simon & 9311.0 & 11768.0 & 20724.0 & 49.6 \\               
\hline                                                              
Sensitivity [\%] & 47.3 & 44.7 & 60.0 & 50.6 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 10 digits}                              
\label{table:Lin_conf_10}                                           
\end{table}                                     


\section{Discussion}
In this project there are three test dataset, one for 1 digit and another for 2 and 10 digits.
The target vector and the estimated target for the test of 1 digit is shown on figure \ref{fig:Lin_fig_1}.
The confusion matrix (CM) is calculated using the MATLAB code. 
The CM of one digit is displayed on table \ref{fig:Lin_conf_1}, The overall accuracy of the linear classification is  calculated to 55.9 \%.
The CM of two digits is displayed on table \ref{fig:Lin_conf_2}, The overall accuracy of the linear classification is  calculated to 51.2 \%.
The CM of ten digits is displayed on table \ref{fig:Lin_conf_10}, The overall accuracy of the linear classification is  calculated to 50.6 \%.
The CM of the classification show that the model has decreasing accuracy for higher numbers of digits.
Generally the linear classifier is not optimal for speaker recognition, this also shows in the accuracy. 
The model is still over the accuracy of 33 \%, which is a random classifier, but not as reliable as the other model, later in the rapport. 
