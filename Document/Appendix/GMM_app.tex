\chapter{Gaussian Mixture Models}
\section{Theory}

Gaussian Mixture Models (GMM) is a way of finding and describing sub-populations in clusters of data points. 
It is done by fitting a specified number of Gaussian distributions to a population of data points.
Each distribution is a component of the model. 
The individual data points are then arranged into clusters based on which model component is most likely given the observed data point.

In this context the data points are features in a multidimensional feature space. Hence the distributions used for the GMM is multivariate Gaussians.

The clustering on basis of likelihood makes GMMs more robust than e.g. K-means, where data points are clustered similarly, but simply on the basis of Euclidean distance to the center of a model. 
This is because treating the components like Gaussians with means and variances instead spheres with uniform probability, gives a more nuanced picture of the strength of a data point’s relationship to a model component. 
It also gives rise to the notion of soft relations to clusters or data points related to more than one cluster.

When fitting the GMM the goal is to maximize the overall likelihood of the model for the entire population.
Firstly it is necessary to determine the number of distribution components the data should be fitted to. 
The number of distributions needed in a GMM greatly depends on the nature and origin(s)
of data.
This, however, does not mean that if a mixture model fits data well with a specific amount of distributions, then this is the actual number of sub-populations, or sources if you will.
Multiple sub-populations could be grouped together, or likewise sub-populations could be split, due to under/over-fitting.
For this reason experimentation is necessary to establish the optimal number of distributions needed to model data.

\fxnote{ What we did for this project (Måske under metode)}

% Afsnit om EM-algoritmen til GMM
\input{EM_alg_app}

\section{Method}
The way GMM was applied in this project was, in simple, to generate a single GMM for each speaker using the training data.
Then the test set was classified by summing the log-likelihoods for each GMM for a number of sequential frames and then selecting the speaker with the highest sum od log-likelihoods.
This approach adds a measure of supervision to an otherwise unsupervised technique.
Further, because classification is done using a set of sequential frames, a temporal element is added.






\section{Results}











