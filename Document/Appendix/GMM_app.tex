\chapter{Gaussian Mixture Models}
\label{sec:GMM}
\section{Theory}

Gaussian Mixture Models (GMM) is a way of finding and describing sub-populations in clusters of data points. 
It is done by fitting a specified number of Gaussian distributions to a population of data points.
Each distribution is a component of the model. 
The individual data points are then arranged into clusters based on which model component is most likely given the observed data point.

In this context the data points are features in a multidimensional feature space. Hence the distributions used for the GMM is multivariate Gaussians.

The clustering on basis of likelihood makes GMMs more robust than e.g. K-means, where data points are clustered similarly, but simply on the basis of Euclidean distance to the center of a model. 
This is because treating the components like Gaussians with means and variances instead spheres with uniform probability, gives a more nuanced picture of the strength of a data pointâ€™s relationship to a model component. 
It also gives rise to the notion of soft relations to clusters or data points related to more than one cluster.

When fitting the GMM the goal is to maximize the overall likelihood of the model for the entire population.
Firstly it is necessary to determine the number of distribution components the data should be fitted to. 
The number of distributions needed in a GMM greatly depends on the nature and origin(s)
of data.
This, however, does not mean that if a mixture model fits data well with a specific amount of distributions, then this is the actual number of sub-populations, or sources if you will.
Multiple sub-populations could be grouped together, or likewise sub-populations could be split, due to under/over-fitting.
For this reason experimentation is necessary to establish the optimal number of distributions needed to model data.

% Afsnit om EM-algoritmen til GMM
\input{EM_alg_app}

\section{Method}
The way GMM was applied in this project was, in simple, to generate a single GMM for each speaker using the training data.
Then the test set was classified by summing the log-likelihoods for each GMM for a number of sequential frames and then selecting the speaker with the highest sum of log-likelihoods.
This approach adds a measure of supervision to an otherwise unsupervised technique.
Further, because classification is done using a set of sequential frames, a temporal element is added.

\subsection{Training}
A GMM was trained for every speaker.
The feature data (both training and test data) has been subjected to PCA, in order to project the data onto a basis that aligned the variance along the dimensions without removing any dimensions. 
Then, for each speaker, a GMM is fitted to the respective speakers' training data, using MATLAB's Statistics Toolbox and the following parameters:

\begin{itemize}

\item
Number of components: \textit{8}

\item
Covariance matrix: \textit{Full}

\item
Initialization: \textit{Random}

\end{itemize}

\subsection{Classification}
Classification is done on the test data is done on super frames of 100 sequential frames.
Each frame assumed to be independent. For this reason the probability of on super frame given a specific model equals the product of the probabilities for each frame.
\begin{equation}
p(\mathbf{x}_{n..n+100} | C_k) = 
\prod_{i=n}^{n+100}
p(\mathbf{x}_{i} | C_k) 
\end{equation}

Products of many likelihoods have a tendency to result in very low numbers. 
Also multiplication is usually more computational intensive than addition.
So in order to speed up calculations and alleviate numerical instability due to underflow the negative log-likelihoods are used.

\begin{equation}
p(\mathbf{x}_{n..n+100} | C_k) = 
\sum_{i=n}^{n+100}
	-\log(
		p(\mathbf{x}_{i} | C_k)
	) 
\end{equation}

The model with the largest sum is chosen as winner.
\begin{equation}
\text{class} = \argmax_j
	\left\lbrace 
		\sum_{i=n}^{n+100}
		-\log(
			p(\mathbf{x}_{i} | C_j)
		) 
	\right\rbrace  
\end{equation}


\section{Results}
Using GMM has been tried with different data complexities.
Most notably, with speakers uttering the same single digit ("\textit{ZERO}"), two different digits ("\textit{ZERO}" and "\textit{ONE}"), and ten different digits ("\textit{ZERO}", "\textit{ONE}" through "\textit{NINE}").

\subsection{Single digit:}

\begin{figure}[H]
\centering
\includegraphics{GMM_1digit_8cent_3speak}
\caption{Results of using GMM with 3 speakers, 8 centers per model and 1 digit spoken}
\label{fig:GMM_fig_1}
\end{figure}

\begin{table}[H]                                                   
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 3400.0 & 0.0 & 0.0 & 100.0 \\                      
\hline                                                              
Estimate Mose & 54.0 & 2946.0 & 0.0 & 98.2 \\                       
\hline                                                              
Estimate Simon & 0.0 & 508.0 & 3454.0 & 87.2 \\                     
\hline                                                              
Sensitivity [\%] & 98.4 & 85.3 & 100.0 & 94.6 \\                    
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 1 digit}                                
\label{table:GMM_conf_1}                                            
\end{table} 


\subsection{Two digits:}
\begin{figure}[H]
\centering
\includegraphics{GMM_2digit_8cent_3speak}
\caption{Results of using GMM with 3 speakers, 8 centers per model and 2 digits spoken}
\label{fig:GMM_fig_2}
\end{figure}

\begin{table}[H]                                              
\centering                                                     
\begin{tabular}{|l|c|c|c|c|}                                   
\hline                                                         
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                         
Estimate Jacob & 6300.0 & 0.0 & 0.0 & 100.0 \\                 
\hline                                                         
Estimate Mose & 10.0 & 5790.0 & 100.0 & 98.1 \\                
\hline                                                         
Estimate Simon & 600.0 & 1120.0 & 6810.0 & 79.8 \\             
\hline                                                         
Sensitivity [\%] & 91.2 & 83.8 & 98.6 & 91.2 \\                
\hline                                                         
\end{tabular}                                                  
\caption{Confusion matrix - 2 digits}                          
\label{table:GMM_conf_2}                                       
\end{table} 


\subsection{Ten digits:}

\begin{figure}[H]
\centering
\includegraphics{GMM_10digit_8cent_3speak}
\caption{Results of using GMM with 3 speakers, 8 centers per model and 10 digits spoken}
\label{fig:GMM_fig_10}
\end{figure}

\begin{table}[H]                          
\centering                                                     
\begin{tabular}{|l|c|c|c|c|}                                   
\hline                                                         
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                         
Estimate Jacob & 33000.0 & 1300.0 & 600.0 & 94.6 \\            
\hline                                                         
Estimate Mose & 1159.0 & 29341.0 & 3900.0 & 85.3 \\            
\hline                                                         
Estimate Simon & 400.0 & 3918.0 & 30059.0 & 87.4 \\            
\hline                                                         
Sensitivity [\%] & 95.5 & 84.9 & 87.0 & 89.1 \\                
\hline                                                         
\end{tabular}                                                  
\caption{Confusion matrix - 10 digits}                         
\label{table:GMM_conf_10}                                      
\end{table}

\section{Discussion}
In this project there the three test dataset.
The CM of one digit is displayed on table \ref{table:GMM_conf_1}, The overall accuracy of the linear classification is  calculated to 94.6 \%.
The CM of two digits is displayed on table \ref{table:GMM_conf_2}, The overall accuracy of the linear classification is  calculated to 91.2 \%.
The CM of ten digits is displayed on table \ref{table:GMM_conf_10}, The overall accuracy of the linear classification is  calculated to 89.1 \%.
The CM of the classification show that the model has decreasing accuracy for higher numbers of digits.
The reason of this decreasing accuracy is the rising complexity of the dataset when the number of digits grow.
The GMM model gives a very good result comparable with that of the ANN model. 
concluding that the GMM model can be used as an reliable speaker recognition classifier.
The GMM model could be optimized with better fits, if the calculations had more computer power.












