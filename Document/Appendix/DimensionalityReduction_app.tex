\chapter{Dimensionality Reduction}
\section{Theory}
Dimensionality reduction is used to reduce the number of variables of the feature extraction.
There is two methods to make Dimensionality reduction, the first method is Fisher's linear discriminant model.
Fisher's model takes a D-dimensional input vector \textbf{x} and project to one dimension bye the equation:
\begin{equation}
y = \mathbf{w}^T \mathbf{x}.
\label{eq:fisher}
\end{equation} 

The projection down to one dimension leads to loss of information, and the data even if it was well separated in D-dimensions, can become overlapping when only viewed in one dimension.
The other method is Principal component analysis (PCA), which can be used to make a dimensionality reduction of the dataset.
PCA works bye orthogonal project the of the data onto a lower dimensional linear space.
The new space is called principal subspace and are made so that the variance of the projected data is maximized. 

\begin{figure}[H]
\centering
\includegraphics{Figure12_2_pdf}
\caption{Principal component analysis orthogonal project the of the data onto a lower dimensional linear space, This is shown on the figure. \fxnote{ref til bishop}}
\label{fig:dim_PCA_book}
\end{figure}

In this project PCA is used to make the dimensional reduction focusing on the maximum variance approach.

\subsection{Maximum Variance}
Consider a dataset of observations $ \left\lbrace \mathbf{x}_n \right\rbrace $ with $ n = 1,...,N $, and the data vector is a Euclidean variable with dimension D. 
The dataset can be projected to lower dimensional space with dimension $ M<D $ 	while maximizing the variance of the projected data. 
The projected space of dimension $ M $ is based on the eigenvectors $ \mathbf{u}_1, ..., \mathbf{u}_M $.
Consider a projection onto $ M=1 $ dimension, the projected space is spanned by the vector $ \mathbf{u}_1 $. 
Then each data point $ \mathbf{x}_n $ is projected onto a scalar $ \mathbf{u}_1^T \mathbf{x}_n $. The mean of the sample set is given by $ \overline{\mathbf{x}} \dfrac{1}{N}\sum_{n=1}^{N} \mathbf{x}_n $.
The calculate the variance of the projected data is given by
\begin{equation}
\dfrac{1}{N}\sum_{n=1}^{N}\left(\mathbf{u}_1^T \mathbf{x}_n-\mathbf{u}_1^T \overline{\mathbf{x}} \right)^2 = \mathbf{u}_1^T \mathbf{S} \mathbf{u}_1
\end{equation}
Where \textbf{S} is the covariance matrix. 
Then maximize the projected variance $ \mathbf{u}_1^T \mathbf{S} \mathbf{u}_1 $ with respect to $ \mathbf{u}_1 $.
Which show that the $ \mathbf{u}_1 $ is an eigenvector of \textbf{S}.
\begin{equation}
\mathbf{S}\mathbf{u}_1 = \lambda_1 \mathbf{u}_1
\end{equation}
Multiplying by $ \mathbf{u}_1 $ on the left.
\begin{equation}
\mathbf{u}_1^T \mathbf{S}\mathbf{u}_1 = \lambda_1
\end{equation}
The maximum of the variance is $ \mathbf{u}_1 $ is set equal to the eigenvector having the largest eigenvalue $ \lambda_1 $.
This eigenvector is called the first principal component. 
When $ M $ is set higher than 1, additional principal component can be defined by choosing each new direction that maximizes the projected variance.
This can be used to remove the dimensions that have small eigenvalue and therefore little information.         


\section{Method}
\fxnote{Insert MATLAB methode}

\section{Results}

\subsection{Single digit:}

\subsection{Two digits:}

\subsection{Ten digits:}

\section{Discussion}

\fxnote{Diskuter resultater og evt forbedringer}