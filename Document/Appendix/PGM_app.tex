\chapter{Probabilistic Generative Models}
In this part of the appendix, describes the Probabilistic Generative models (PGM) and how it is used on the dataset.

\section{Theory} 
The model is a multivariate normal distribution, which means that each class corresponds to a unique distribution.
\begin{equation}
p(\mathbf{x}|C_k)=
\mathcal{N}(\mathbf{x};\mathbf{\mu}_k, \; \Sigma_k) =
 \dfrac{1}{(2\pi)^{D/2}} \dfrac{1}{\left|\mathbf{\Sigma} \right|^{1/2}} 
 \mathtt{exp} \left\lbrace -\dfrac{1}{2} (\mathbf{x}-\mathbf{\mu}_k)^T \mathbf{\Sigma}^{-1} (\mathbf{x}-\mathbf{\mu}_k) \right\rbrace
\label{eq:gauss_dist} 
\end{equation}
The probability distribution is used in addition to the prior probability of each class, This enables the possibility to calculate the posterior probability of a class given a feature vector from Bayes theorem.
\begin{equation}
p(C_k |\mathbf{x}) =
\dfrac{p(\mathbf{x}|C_k) p(C_k)}
{\sum_j p(\mathbf{x}|C_j) p(C_j)}
\label{eq:posteriorP}
\end{equation}
The prior probabilities for each class are approximated to be equal, to indicate that class \textit{k} is as frequent as class \textit{j}, which leads to a simplification of Bayes theorem.
\begin{equation}
p(C_k) = p(C_j) = \alpha \Longrightarrow p(C_k |\mathbf{x}) = 
\dfrac
{\alpha \cdot p(\mathbf{x}|C_k) }
{\alpha \cdot \sum_j p(\mathbf{x}|C_j) } = 
\dfrac{p(\mathbf{x}|C_k)}
{\sum_j p(\mathbf{x}|C_j)}
\label{eq:posteriorPsimple}
\end{equation}



\section{Method}
To make the generative models the mean vector and covariance matrix for each class was calculated using all the training data for the respective classes.
\begin{eqnarray}
\bm{\mu}_i= \dfrac{1}{N} \sum_{j=1}^{N} \mathbf{x}_{kj} \\
\bm{\Sigma}_k =\dfrac{1}{N} \sum_{j=1}^{N} (\mathbf{x}_{kj}-\bm{\mu}_k) \cdot (\mathbf{x}_{kj}--\bm{\mu}_k)
\end{eqnarray}
The class-conditional densities, for all samples and for all speaker models was then calculated using (\ref{eq:gauss_dist}).
Then the posterior probability for all classes are calculated using Bayes' theorem (\ref{eq:posteriorP}) and assuming uniform class priors.
Finally the samples were classified by comparing posterior probabilities for all classes and selecting the greatest.




\section{Results}

\subsection{1 digit:}

\begin{figure}[H]
\centering
\includegraphics{PGM_1digit}
\caption{Results of using PGM classifiers and one digit spoken}
\label{fig:PGM_1dig}
\end{figure}

\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 2027.0 & 313.0 & 391.0 & 74.2 \\                   
\hline                                                              
Estimate Mose & 569.0 & 2199.0 & 794.0 & 61.7 \\                    
\hline                                                              
Estimate Simon & 858.0 & 942.0 & 2269.0 & 55.8 \\                   
\hline                                                              
Sensitivity [\%] & 58.7 & 63.7 & 65.7 & 62.7 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 1 digit}                                
\label{table:PGM_conf_1}                                            
\end{table}       


\subsection{2 digits:}

\begin{figure}[H]
\centering
\includegraphics{PGM_2digit}
\caption{Results of using PGM classifiers and one digit spoken}
\label{fig:PGM_2dig}
\end{figure}

\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 3471.0 & 585.0 & 1051.0 & 68.0 \\                  
\hline                                                              
Estimate Mose & 1567.0 & 4213.0 & 1500.0 & 57.9 \\                  
\hline                                                              
Estimate Simon & 1872.0 & 2112.0 & 4359.0 & 52.2 \\                 
\hline                                                              
Sensitivity [\%] & 50.2 & 61.0 & 63.1 & 58.1 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 2 digits}                               
\label{table:PGM_conf_2}                                            
\end{table}   


\subsection{10 digits:}

\begin{figure}[H]
\centering
\includegraphics{PGM_10digit}
\caption{Results of using PGM classifiers and ten digits spoken}
\label{fig:PGM_10dig}
\end{figure}


\begin{table}[H]                                                    
\centering                                                          
\begin{tabular}{|l|c|c|c|c|}                                        
\hline                                                              
  & Speaker Jacob & Speaker Mose & Speaker Simon & Precision [\%] \\
\hline                                                              
Estimate Jacob & 18641.0 & 5089.0 & 5091.0 & 64.7 \\                
\hline                                                              
Estimate Mose & 9440.0 & 20760.0 & 9570.0 & 52.2 \\                 
\hline                                                              
Estimate Simon & 6478.0 & 8710.0 & 19898.0 & 56.7 \\                
\hline                                                              
Sensitivity [\%] & 53.9 & 60.1 & 57.6 & 57.2 \\                     
\hline                                                              
\end{tabular}                                                       
\caption{Confusion matrix - 10 digits}                              
\label{table:PGM_conf_10}                                           
\end{table}

\section{Discussion}
Overall the accuracy of PGM ranging from $ 62.7 \% $ for single digit 
\footnote{(See table \ref{table:PGM_conf_1})} 
to $ 57.2 \% $ for all ten digits
\footnote{(See table \ref{table:PGM_conf_10})}, is significant relative to a random classification of $ 33 \% $.
It is, though, not good enough for reliable classification. 

This is to be expected, as modelling all the sounds in a word or digit precisely, let alone 10 of them, with a single multivariate Gaussian is not feasible.

 


