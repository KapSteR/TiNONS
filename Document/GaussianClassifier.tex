%!TEX root = Main.tex
\section*{Gaussian Mixture Models}
Gaussian Mixture Models (GMM) is a way of finding and describing sub-populations in clusters of data points.
It is done by fitting a specified number of Gaussian distributions to a population of data points.
Each distribution is a component of the model. 
The individual data points are then arranged into clusters based on which model component is most likely given the observed data point.

The distributions of the mixture model are fitted to data by iteratively employing Expectation Maximization (EM).

\subsection*{The EM Algotrithm}
In EM a mixture model is iteratively updated by esitimating model parameters $\bm{\mu}_k $ (component means) and $ \bm{\Sigma}_k $ (component covariance matrix) based on current assignment of responability and then reassigning responability based on new model parameters, starting out with a random guess of $\bm{\mu}_k \text{ and } \bm{\Sigma}_k \text{ for } k=1..K$ 


\subsubsection*{Initialization:}
\begin{enumerate}
\item
Choose initial estimates for model parameters $ \mathbf{\pi}_{k}, \mathbf{\mu}_{k}, \mathbf{\Sigma}_{k} $.

\begin{itemize}

	\item
	$ k $ is the component number out of K.

	\item
	$ \pi_{k} $  is the weight of the \textit{k}th component.

	\item
	$ \mathbf{\mu}_{k}$ is the mean of the \textit{k}th component.

	\item
	$ \mathbf{\Sigma}_{k} $ is the covariance matrix of the \textit{k}th component.

	\end{itemize}


\item
Compute the initial log-likelihood og the model

\begin{equation} \label{eq:loglikeGMM}
\ln p\left(X | \mathbf{\mu}, \mathbf{\Sigma}, \pi\right) = 
\sum_{n=1}^{N} \ln \sum_{k=1}^{N} \pi_{k}\mathcal{N}(\mathbf{x}_{n}|\mathbf{\mu}_{k},\mathbf{\Sigma}_{k})
\end{equation}

\end{enumerate}

\subsubsection*{E-step:}
Calculate the probability of each point in each component in order to assing responsibility

\begin{equation}
\gamma_{nk} = 
\frac
{\pi_{k}\mathcal{N}(\mathbf{x}_{n}|\mathbf{\mu}_{k},\mathbf{\Sigma}_{k})}
{\sum_{j=1}^{K} \pi_{n}\mathcal{N}(\mathbf{x}_{n}|\mu_{j},\mathbf{\Sigma}_{j})}
\end{equation}

\subsubsection*{M-step:}
Estimate new guesses for $ \mathbf{\pi}_{k}, \mathbf{\mu}_{k}, \mathbf{\Sigma}_{k} $.

\begin{equation}
\mathbf{\mu}_{k}^{new} = 
\frac{1}{N_{k}}
\sum_{n=1}^{N} 
\gamma_{nk}
\mathbf{x}_{k}
\end{equation}

\begin{equation}
\mu_{k}^{new} = 
\frac{1}{N_{k}}
\sum_{n=1}^{N} 
\gamma_{nk}
(\mathbf{x}_{k} - \mathbf{\mu}_{k}^{new})
(\mathbf{x}_{k} - \mathbf{\mu}_{k}^{new})^{T}
\end{equation}

\begin{equation}
\pi_{k}^{new} =
\frac
{N_{k}}
{N}
\end{equation}

\subsubsection*{Convergence check:}

Recalculate log likelihood using Equation \ref{eq:loglikeGMM}.
If the log likelihood has not changed more than some predetermined threshold stop iteration.
Otherwise continue from E-step.

\paragraph*{Training}
GMM is a tyoe of unsupervised machine learning, and is a first hand not usefull in our project, but by training a GMM for each speaker based on the pre-classfied trainging set, you end up with a very effective model.


Then, for each speaker, a GMM is fitted to the respective speakers' training data, using MATLAB's Statistics Toolbox.

\paragraph*{Result}
